# v6 Testing Framework

## Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up API key
cp .env.example .env
# Edit .env and add your OpenAI API key

# 3. Run tests (45 tests, ~23 minutes at 30s/test)
python run_tests_v6.py

# 4. Score results
python score_results_v6.py --csv
```

## Overview

v6 testing validates the **expanded relevance logic** using 45 real posts from the production pipeline that were incorrectly marked as `not_relevant` in v5.

### What v6 Fixes

| Issue | v5 Behavior | v6 Expected |
|-------|-------------|-------------|
| Bariatric posts without PBH | â†’ not_relevant | â†’ borderline |
| PBH treatments (acarbose, etc.) | â†’ borderline (weak context) | â†’ relevant |
| GLP-1s + bariatric context | â†’ not_relevant | â†’ relevant |

### Test Categories

| Category | Count | Expected Relevance |
|----------|-------|-------------------|
| bariatric_context_only | 23 | borderline |
| weak_bariatric | 12 | borderline |
| glp1_only | 7 | borderline |
| pbh_mention | 3 | relevant |

## Directory Structure

```
testing/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ V6_TESTING_ARCHITECTURE.md         # Detailed architecture docs
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .env.example                       # API key template
â”‚
â”œâ”€â”€ # Data Preparation
â”œâ”€â”€ extract_test_candidates.py         # Identify test posts from CSV
â”œâ”€â”€ csv_to_normalized.py               # Convert CSV â†’ normalized JSON
â”œâ”€â”€ generate_expected_outcomes.py      # Create expected outcomes manifest
â”‚
â”œâ”€â”€ # Test Execution
â”œâ”€â”€ run_tests_v6.py                    # Run enrichment via Chat Completions
â”‚
â”œâ”€â”€ # Analysis
â”œâ”€â”€ score_results_v6.py                # Score results against expectations
â”‚
â”œâ”€â”€ # Data Files
â”œâ”€â”€ test_candidates/
â”‚   â”œâ”€â”€ all_test_candidates.csv        # 64 posts with relevant triggers
â”‚   â””â”€â”€ changed_relevance_candidates.csv # 45 posts that should change
â”œâ”€â”€ normalized_inputs/                 # 45 normalized JSON test inputs
â”œâ”€â”€ enriched_outputs/                  # Enriched JSON outputs (after running)
â”œâ”€â”€ v6_expected_outcomes.json          # Expected values for scoring
â””â”€â”€ reports/                           # CSV test reports
```

## Detailed Usage

### Step 1: Verify Setup (Already Done)

Test candidates and normalized inputs were already created:

```bash
# These were run during setup:
python extract_test_candidates.py    # Found 45 test candidates
python csv_to_normalized.py          # Created 45 normalized JSON files
python generate_expected_outcomes.py # Created expected outcomes manifest
```

### Step 2: Run Tests

```bash
# Run all 45 tests (default 30s delay between calls)
python run_tests_v6.py

# Run faster (10s delay) - may hit rate limits
python run_tests_v6.py --delay 10

# Run single test
python run_tests_v6.py --source-id t3_1ph44gy

# Run first 5 tests only
python run_tests_v6.py --limit 5
```

**Expected runtime:** ~23 minutes for 45 tests at 30s/test

### Step 3: Score Results

```bash
# Basic scoring with summary
python score_results_v6.py

# Verbose output (per-test details)
python score_results_v6.py --verbose

# Generate CSV report
python score_results_v6.py --csv
```

## Success Criteria

| Metric | Target | Description |
|--------|--------|-------------|
| Relevance accuracy | â‰¥95% | Primary v6 validation metric |
| bariatric_context populated | 100% | Must never be empty/NaN |
| False negatives | 0 | No bariatric posts â†’ not_relevant |

## Output Example

```
ğŸ“Š PRIMARY METRIC: Relevance Label Accuracy
   âœ… Correct: 43/45 (95.6%)
   âŒ Wrong: 2/45 (4.4%)

ğŸ“Š BY CATEGORY:
   bariatric_context_only (23 tests):
      Relevance: 22/23 (96%)
   weak_bariatric (12 tests):
      Relevance: 12/12 (100%)
   glp1_only (7 tests):
      Relevance: 6/7 (86%)
   pbh_mention (3 tests):
      Relevance: 3/3 (100%)

ğŸ‰ v6 VALIDATION: PASSED (â‰¥95% relevance accuracy)
```

## Troubleshooting

### API Key Issues
```
âŒ Error: OPENAI_API_KEY not found
```
â†’ Copy `.env.example` to `.env` and add your API key

### Rate Limiting
```
âŒ API Error: RateLimitError
```
â†’ Increase delay: `python run_tests_v6.py --delay 60`

### Missing Files
```
âŒ Error: normalized_inputs directory not found
```
â†’ Run: `python csv_to_normalized.py`

## Files for Dev Team

When deploying v6 to production, use:

1. **System Prompt:** `../enrichment/openai_assistant_system_prompt_v6_with_dictionary.md`
2. **Response Schema:** `../enrichment/openai_assistant_response_format_v6.json`
3. **Reference:** `reference_schemas/PBH_SIGNAL_ENRICHMENT_SCHEMA_v6.csv`
